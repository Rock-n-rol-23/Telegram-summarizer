"""
–î–≤—É—Ö–ø—Ä–æ—Ö–æ–¥–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Å –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ñ–∞–∫—Ç–æ–≤
"""

import json
import logging
import re
from typing import Dict, List, Optional, Literal, Union
from pathlib import Path

from .fact_extractor import extract_key_facts, select_must_keep_sentences
from quality.quality_checks import (
    validate_numbers_preserved, 
    check_summary_quality,
    validate_json_structure,
    trim_to_length
)

logger = logging.getLogger(__name__)

class SummarizationPipeline:
    """–î–≤—É—Ö—Ñ–∞–∑–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –∫–∞—á–µ—Å—Ç–≤–∞"""
    
    def __init__(self, groq_client, fallback_summarizer=None):
        self.groq_client = groq_client
        self.fallback_summarizer = fallback_summarizer
        self.prompts = self._load_prompts()
    
    def _load_prompts(self) -> Dict[str, str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞"""
        prompts_file = Path(__file__).parent.parent / "prompts" / "llm_summarize.txt"
        
        try:
            with open(prompts_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # –ü–∞—Ä—Å–∏–º –ø—Ä–æ–º–ø—Ç—ã
            prompts = {}
            current_key = None
            current_content = []
            
            for line in content.split('\n'):
                if line.startswith(('SYSTEM_PROMPT_', 'USER_PROMPT_', 'FIXUP_PROMPT:', 'FALLBACK_PROMPT:')):
                    if current_key:
                        prompts[current_key] = '\n'.join(current_content).strip()
                    current_key = line.rstrip(':')
                    current_content = []
                else:
                    current_content.append(line)
            
            if current_key:
                prompts[current_key] = '\n'.join(current_content).strip()
            
            logger.info(f"Loaded {len(prompts)} prompts")
            return prompts
        
        except Exception as e:
            logger.error(f"Failed to load prompts: {e}")
            return self._get_fallback_prompts()
    
    def _get_fallback_prompts(self) -> Dict[str, str]:
        """–ü—Ä–æ–º–ø—Ç—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª—Å—è"""
        return {
            'SYSTEM_PROMPT_PHASE_A': """–¢—ã ‚Äî –ø–µ–¥–∞–Ω—Ç–∏—á–Ω—ã–π —Ä–µ–¥–∞–∫—Ç–æ—Ä-—Å–∞–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä. –°–æ—Ö—Ä–∞–Ω—è–π –≤—Å–µ —á–∏—Å–ª–∞, –¥–∞—Ç—ã, –ø—Ä–æ—Ü–µ–Ω—Ç—ã, –¥–µ–Ω–µ–∂–Ω—ã–µ —Å—É–º–º—ã –∏ –∏–º–µ–Ω–∞. –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–º JSON —Å –ø–æ–ª—è–º–∏: bullets, key_facts, entities, uncertainties.""",
            'USER_PROMPT_PHASE_A': """LANGUAGE={language}\nFORMAT={format}\nTEXT=<<<\n{text}\n>>>""",
            'SYSTEM_PROMPT_PHASE_B': """–°–æ–∑–¥–∞–π —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ–µ –∏–∑–ª–æ–∂–µ–Ω–∏–µ. –í–∫–ª—é—á–∏ –≤—Å–µ —Ñ–∞–∫—Ç—ã. –í –∫–æ–Ω—Ü–µ –¥–æ–±–∞–≤—å –±–ª–æ–∫ "üî¢ –¶–∏—Ñ—Ä—ã –∏ —Ñ–∞–∫—Ç—ã".""",
            'USER_PROMPT_PHASE_B': """JSON_DATA=<<<\n{json_data}\n>>>\nFORMAT={format}\nLANGUAGE={language}""",
            'FALLBACK_PROMPT': """–°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–æ–µ –∏–∑–ª–æ–∂–µ–Ω–∏–µ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤—Å–µ—Ö —á–∏—Å–µ–ª –∏ –¥–∞—Ç. –í –∫–æ–Ω—Ü–µ –±–ª–æ–∫ "üî¢ –¶–∏—Ñ—Ä—ã –∏ —Ñ–∞–∫—Ç—ã"."""
        }
    
    def _split_into_chunks(self, text: str, max_chars: int = 2800) -> List[str]:
        """–†–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        if len(text) <= max_chars:
            return [text]
        
        chunks = []
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —Ä–∞–∑–±–∏—Ç—å –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º
        header_pattern = r'\n\n(?=[–ê-–Ø–ÅA-Z].*:|\d+\.|\*|#)'
        sections = re.split(header_pattern, text)
        
        current_chunk = ""
        for section in sections:
            if len(current_chunk + section) <= max_chars:
                current_chunk += section
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = section
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        # –ï—Å–ª–∏ —á–∞–Ω–∫–∏ –≤—Å–µ –µ—â–µ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ, —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
        final_chunks = []
        for chunk in chunks:
            if len(chunk) <= max_chars:
                final_chunks.append(chunk)
            else:
                sentences = re.split(r'[.!?]+', chunk)
                sub_chunk = ""
                for sentence in sentences:
                    if len(sub_chunk + sentence) <= max_chars:
                        sub_chunk += sentence + ". "
                    else:
                        if sub_chunk:
                            final_chunks.append(sub_chunk.strip())
                        sub_chunk = sentence + ". "
                
                if sub_chunk:
                    final_chunks.append(sub_chunk.strip())
        
        return final_chunks
    
    async def _llm_phase_a(self, text: str, language: str, format_type: str, 
                          target_chars: int, must_keep_indexes: List[int]) -> Optional[Dict]:
        """–ü–µ—Ä–≤–∞—è —Ñ–∞–∑–∞ LLM - –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ JSON"""
        
        system_prompt = self.prompts.get('SYSTEM_PROMPT_PHASE_A', '')
        user_prompt = self.prompts.get('USER_PROMPT_PHASE_A', '').format(
            language=language,
            format=format_type,
            target_chars=target_chars,
            must_keep_indexes=must_keep_indexes,
            text=text
        )
        
        try:
            response = self.groq_client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1,
                max_tokens=2000,
                response_format={"type": "json_object"}
            )
            
            result_text = response.choices[0].message.content
            json_data = json.loads(result_text)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            is_valid, errors = validate_json_structure(json_data)
            if not is_valid:
                logger.warning(f"Invalid JSON structure: {errors}")
                return None
            
            return json_data
        
        except Exception as e:
            logger.error(f"Phase A failed: {e}")
            return None
    
    async def _llm_phase_b(self, json_data: Dict, language: str, 
                          format_type: str, target_chars: int) -> Optional[str]:
        """–í—Ç–æ—Ä–∞—è —Ñ–∞–∑–∞ LLM - –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        
        system_prompt = self.prompts.get('SYSTEM_PROMPT_PHASE_B', '').format(
            format=format_type, language=language, target_chars=target_chars
        )
        user_prompt = self.prompts.get('USER_PROMPT_PHASE_B', '').format(
            json_data=json.dumps(json_data, ensure_ascii=False, indent=2),
            target_chars=target_chars,
            format=format_type,
            language=language
        )
        
        try:
            response = self.groq_client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.2,
                max_tokens=3000
            )
            
            return response.choices[0].message.content
        
        except Exception as e:
            logger.error(f"Phase B failed: {e}")
            return None
    
    def _merge_json_chunks(self, json_chunks: List[Dict]) -> Dict:
        """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ JSON –¥–∞–Ω–Ω—ã—Ö –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —á–∞–Ω–∫–æ–≤"""
        merged = {
            'bullets': [],
            'key_facts': [],
            'entities': {'ORG': [], 'PERSON': [], 'GPE': []},
            'uncertainties': []
        }
        
        seen_facts = set()  # –î–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
        
        for chunk_data in json_chunks:
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º bullets
            merged['bullets'].extend(chunk_data.get('bullets', []))
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º key_facts —Å –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π
            for fact in chunk_data.get('key_facts', []):
                fact_key = fact.get('value_raw', '').strip().lower()
                if fact_key and fact_key not in seen_facts:
                    seen_facts.add(fact_key)
                    merged['key_facts'].append(fact)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º entities
            for entity_type in ['ORG', 'PERSON', 'GPE']:
                entities_list = chunk_data.get('entities', {}).get(entity_type, [])
                merged['entities'][entity_type].extend(entities_list)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º uncertainties
            merged['uncertainties'].extend(chunk_data.get('uncertainties', []))
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ entities
        for entity_type in merged['entities']:
            merged['entities'][entity_type] = list(set(merged['entities'][entity_type]))
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ bullets –¥–æ —Ä–∞–∑—É–º–Ω–æ–≥–æ
        if len(merged['bullets']) > 12:
            merged['bullets'] = merged['bullets'][:12]
        
        return merged
    
    def _create_fallback_summary(self, text: str, language: str, 
                                format_type: str, target_chars: int) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å—É–º–º–∞—Ä–∏ –±–µ–∑ LLM (fallback)"""
        logger.info("Creating fallback summary without LLM")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∞–∫—Ç—ã
        facts = extract_key_facts(text, language)
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–µ –∏–∑–ª–æ–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ TextRank –∏–ª–∏ –ø–µ—Ä–≤—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è + –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ
        must_keep = select_must_keep_sentences(facts)
        selected_sentences = []
        
        # –°–Ω–∞—á–∞–ª–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        for i in must_keep:
            if i < len(sentences):
                selected_sentences.append(sentences[i])
        
        # –ü–æ—Ç–æ–º –¥–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–≤—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–æ –Ω—É–∂–Ω–æ–π –¥–ª–∏–Ω—ã
        current_length = sum(len(s) for s in selected_sentences)
        for i, sentence in enumerate(sentences):
            if i not in must_keep and current_length < target_chars * 0.7:
                selected_sentences.append(sentence)
                current_length += len(sentence)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç
        if format_type == 'bullets':
            main_text = '\n'.join(f"‚Ä¢ {s}" for s in selected_sentences[:8])
        else:
            main_text = '. '.join(selected_sentences) + '.'
        
        # –î–æ–±–∞–≤–ª—è–µ–º –±–ª–æ–∫ —Å —Ü–∏—Ñ—Ä–∞–º–∏
        facts_block = self._create_facts_block(facts)
        
        result = f"{main_text}\n\n{facts_block}"
        return trim_to_length(result, target_chars)
    
    def _create_facts_block(self, facts: Dict) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –±–ª–æ–∫–∞ —Å —Ü–∏—Ñ—Ä–∞–º–∏ –∏ —Ñ–∞–∫—Ç–∞–º–∏"""
        facts_lines = []
        
        # –î–µ–Ω–µ–∂–Ω—ã–µ —Å—É–º–º—ã
        for money in facts.get('money', []):
            facts_lines.append(f"‚Äî {money['raw']}")
        
        # –î—Ä—É–≥–∏–µ —á–∏—Å–ª–∞
        for sent_data in facts.get('sentences_with_numbers', []):
            for num in sent_data['numbers']:
                if num['unit'] not in ['RUB', 'USD', 'EUR']:  # –ù–µ –¥—É–±–ª–∏—Ä—É–µ–º –≤–∞–ª—é—Ç—ã
                    facts_lines.append(f"‚Äî {num['raw']}")
        
        # –î–∞—Ç—ã
        for date in facts.get('dates', []):
            facts_lines.append(f"‚Äî {date['raw']}")
        
        if facts_lines:
            return "üî¢ –¶–∏—Ñ—Ä—ã –∏ —Ñ–∞–∫—Ç—ã:\n" + '\n'.join(facts_lines[:10])
        else:
            return "üî¢ –¶–∏—Ñ—Ä—ã –∏ —Ñ–∞–∫—Ç—ã: –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã"
    
    async def summarize_text_pipeline(self, text: str, lang: Literal["ru", "en"] = "ru", 
                                    target_chars: int = 1000, 
                                    format_type: Literal["bullets", "paragraph", "structured"] = "bullets") -> Dict:
        """
        –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–≤—É—Ö–ø—Ä–æ—Ö–æ–¥–Ω–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
        
        Returns:
            {
                'success': bool,
                'summary': str,
                'quality_report': dict,
                'method': str  # 'llm_two_phase', 'fallback'
            }
        """
        logger.info(f"Starting summarization pipeline: {len(text)} chars, {lang}, {format_type}")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∞–∫—Ç—ã –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        source_facts = extract_key_facts(text, lang)
        must_keep_sentences = list(select_must_keep_sentences(source_facts))
        
        # –ï—Å–ª–∏ Groq –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback
        if not self.groq_client:
            summary = self._create_fallback_summary(text, lang, format_type, target_chars)
            quality_report = check_summary_quality(text, summary, lang, target_chars)
            
            return {
                'success': True,
                'summary': summary,
                'quality_report': quality_report,
                'method': 'fallback'
            }
        
        try:
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
            chunks = self._split_into_chunks(text, max_chars=2800)
            logger.info(f"Split into {len(chunks)} chunks")
            
            # –§–∞–∑–∞ A –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
            json_chunks = []
            for i, chunk in enumerate(chunks):
                logger.info(f"Processing chunk {i+1}/{len(chunks)}")
                
                chunk_json = await self._llm_phase_a(
                    chunk, lang, format_type, target_chars // len(chunks), must_keep_sentences
                )
                
                if chunk_json:
                    json_chunks.append(chunk_json)
                else:
                    logger.warning(f"Chunk {i+1} failed, skipping")
            
            if not json_chunks:
                logger.error("All chunks failed, falling back")
                summary = self._create_fallback_summary(text, lang, format_type, target_chars)
                quality_report = check_summary_quality(text, summary, lang, target_chars)
                return {
                    'success': True,
                    'summary': summary,
                    'quality_report': quality_report,
                    'method': 'fallback_after_failure'
                }
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º JSON –¥–∞–Ω–Ω—ã–µ
            merged_json = self._merge_json_chunks(json_chunks)
            
            # –§–∞–∑–∞ B - —Ñ–∏–Ω–∞–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞
            final_summary = await self._llm_phase_b(merged_json, lang, format_type, target_chars)
            
            if not final_summary:
                logger.error("Phase B failed, falling back")
                summary = self._create_fallback_summary(text, lang, format_type, target_chars)
            else:
                summary = final_summary
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
            quality_report = check_summary_quality(text, summary, lang, target_chars)
            
            # –ï—Å–ª–∏ –≤–∞–∂–Ω—ã–µ —á–∏—Å–ª–∞ –ø–æ—Ç–µ—Ä—è–Ω—ã, –ø—ã—Ç–∞–µ–º—Å—è –∏—Ö –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å
            if not quality_report['numbers_preserved'] and quality_report['missing_numbers']:
                logger.info("Attempting to recover missing numbers")
                recovery_result = await self._recover_missing_facts(summary, quality_report['missing_numbers'])
                if recovery_result:
                    summary = recovery_result
                    quality_report = check_summary_quality(text, summary, lang, target_chars)
            
            return {
                'success': True,
                'summary': summary,
                'quality_report': quality_report,
                'method': 'llm_two_phase',
                'chunks_processed': len(json_chunks),
                'source_facts': len(source_facts.get('all_numbers', []))
            }
        
        except Exception as e:
            logger.error(f"Pipeline failed: {e}")
            
            # –û–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–π fallback
            summary = self._create_fallback_summary(text, lang, format_type, target_chars)
            quality_report = check_summary_quality(text, summary, lang, target_chars)
            
            return {
                'success': True,
                'summary': summary,
                'quality_report': quality_report,
                'method': 'fallback_on_error',
                'error': str(e)
            }
    
    async def _recover_missing_facts(self, summary: str, missing_facts: List[str]) -> Optional[str]:
        """–ü–æ–ø—ã—Ç–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–æ—Ç–µ—Ä—è–Ω–Ω—ã–µ —Ñ–∞–∫—Ç—ã"""
        
        if not missing_facts:
            return summary
        
        fixup_prompt = self.prompts.get('FIXUP_PROMPT', '').format(
            missing_facts=', '.join(missing_facts)
        )
        
        try:
            response = self.groq_client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[
                    {"role": "system", "content": fixup_prompt},
                    {"role": "user", "content": summary}
                ],
                temperature=0.1,
                max_tokens=1500
            )
            
            return response.choices[0].message.content
        
        except Exception as e:
            logger.error(f"Fact recovery failed: {e}")
            
            # –ü—Ä–æ—Å—Ç–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –∫–æ–Ω–µ—Ü
            facts_addition = "\n\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–∫—Ç—ã: " + ", ".join(missing_facts)
            return summary + facts_addition


def summarize_text_pipeline(text: str, groq_client, lang: Literal["ru", "en"] = "ru",
                           target_chars: int = 1000, 
                           format_type: Literal["bullets", "paragraph", "structured"] = "bullets") -> Dict:
    """
    –§—É–Ω–∫—Ü–∏—è-–æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥—Ä—É–≥–∏—Ö –º–æ–¥—É–ª—è—Ö
    """
    pipeline = SummarizationPipeline(groq_client)
    import asyncio
    
    # –ï—Å–ª–∏ —É–∂–µ –≤ async –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π loop
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # –í async –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–æ—Ä—É—Ç–∏–Ω—É
            return pipeline.summarize_text_pipeline(text, lang, target_chars, format_type)
        else:
            # –°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            return loop.run_until_complete(
                pipeline.summarize_text_pipeline(text, lang, target_chars, format_type)
            )
    except RuntimeError:
        # –ù–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–≥–æ loop
        return asyncio.run(pipeline.summarize_text_pipeline(text, lang, target_chars, format_type))