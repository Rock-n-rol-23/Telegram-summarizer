#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –∏ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–æ–≤
"""
import sys
import os
import asyncio

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.lang import detect_lang, is_ru, is_en
from summarizers.english_sumy import summarize_en
from summarizer import TextSummarizer

# –¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã
RU_TEXT = """
–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç (–ò–ò) —Å—Ç—Ä–µ–º–∏—Ç–µ–ª—å–Ω–æ —Ä–∞–∑–≤–∏–≤–∞–µ—Ç—Å—è –∏ —É–∂–µ —Å–µ–≥–æ–¥–Ω—è –æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ñ–µ—Ä—ã —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–º–ø—å—é—Ç–µ—Ä–∞–º –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ–≥—Ä–æ–º–Ω—ã–µ –º–∞—Å—Å–∏–≤—ã –¥–∞–Ω–Ω—ã—Ö –∏ –≤—ã—è–≤–ª—è—Ç—å –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–æ –±—ã –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –æ–±–Ω–∞—Ä—É–∂–∏—Ç—å —á–µ–ª–æ–≤–µ–∫—É. –í –º–µ–¥–∏—Ü–∏–Ω–µ –ò–ò –ø–æ–º–æ–≥–∞–µ—Ç –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è –Ω–∞ —Ä–∞–Ω–Ω–∏—Ö —Å—Ç–∞–¥–∏—è—Ö, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é, –ø—Ä–µ–≤—ã—à–∞—é—â–µ–π –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≤—Ä–∞—á–µ–π. –í —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Å—Ñ–µ—Ä–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –º–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π –∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Ç–æ—Ä–≥–æ–≤—ã—Ö —Ä–µ—à–µ–Ω–∏–π. –ê–≤—Ç–æ–Ω–æ–º–Ω—ã–µ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω—ã–µ —Å—Ä–µ–¥—Å—Ç–≤–∞, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö –ò–ò, –æ–±–µ—â–∞—é—Ç —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω—É—é –∏–Ω–¥—É—Å—Ç—Ä–∏—é, —Å–¥–µ–ª–∞–≤ –¥–æ—Ä–æ–≥–∏ –±–æ–ª–µ–µ –±–µ–∑–æ–ø–∞—Å–Ω—ã–º–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º–∏.
"""

EN_TEXT = """
Artificial intelligence (AI) is rapidly evolving and already having a significant impact on various aspects of human activity. Machine learning enables computers to analyze vast amounts of data and identify patterns that would be impossible for humans to detect. In healthcare, AI helps diagnose diseases at early stages by analyzing medical images with accuracy exceeding that of doctors. In the financial sector, machine learning algorithms are used to detect fraudulent transactions and automate trading decisions. Autonomous vehicles based on AI technologies promise to revolutionize the transportation industry, making roads safer and more efficient. The integration of AI into everyday life raises important questions about privacy, employment, and the ethical implications of artificial decision-making systems.
"""

SHORT_RU_TEXT = "–≠—Ç–æ –∫–æ—Ä–æ—Ç–∫–∏–π —Ä—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."
SHORT_EN_TEXT = "This is a short English text for testing purposes."

TEXT_WITH_LINKS = """
Recent studies (https://example.com/study1) show that climate change is accelerating faster than previously predicted. The research conducted by MIT (https://mit.edu/research) indicates significant temperature increases. For more information, visit https://climate.gov for official data.
"""

async def test_language_detection():
    """–¢–µ—Å—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏ —è–∑—ã–∫–∞"""
    print("=== –¢–µ—Å—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏ —è–∑—ã–∫–∞ ===")
    
    # –†—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç
    ru_detected = detect_lang(RU_TEXT)
    print(f"RU —Ç–µ–∫—Å—Ç: {ru_detected} (–æ–∂–∏–¥–∞–µ—Ç—Å—è: ru)")
    assert ru_detected == 'ru', f"–û–∂–∏–¥–∞–ª—Å—è ru, –ø–æ–ª—É—á–µ–Ω {ru_detected}"
    
    # –ê–Ω–≥–ª–∏–π—Å–∫–∏–π —Ç–µ–∫—Å—Ç
    en_detected = detect_lang(EN_TEXT)
    print(f"EN —Ç–µ–∫—Å—Ç: {en_detected} (–æ–∂–∏–¥–∞–µ—Ç—Å—è: en)")
    assert en_detected == 'en', f"–û–∂–∏–¥–∞–ª—Å—è en, –ø–æ–ª—É—á–µ–Ω {en_detected}"
    
    # –ö–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã
    short_ru = detect_lang(SHORT_RU_TEXT)
    short_en = detect_lang(SHORT_EN_TEXT)
    print(f"–ö–æ—Ä–æ—Ç–∫–∏–π RU: {short_ru}, –ö–æ—Ä–æ—Ç–∫–∏–π EN: {short_en}")
    
    print("‚úì –î–µ—Ç–µ–∫—Ü–∏—è —è–∑—ã–∫–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ\n")

def test_english_summarizer():
    """–¢–µ—Å—Ç –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä–∞"""
    print("=== –¢–µ—Å—Ç –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä–∞ ===")
    
    # –î–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    summary = summarize_en(EN_TEXT, max_sentences=5)
    print(f"EN —Å–∞–º–º–∞—Ä–∏ –¥–ª–∏–Ω–æ–π {len(summary)} —Å–∏–º–≤–æ–ª–æ–≤:")
    print(summary)
    print()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∏
    assert len(summary) > 50, "–°–∞–º–º–∞—Ä–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ"
    assert summary.count('‚Ä¢') >= 3, "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±—É–ª–ª–µ—Ç–æ–≤"
    assert 'AI' in summary or 'artificial' in summary.lower(), "–ö–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç"
    
    # –ö–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç
    short_summary = summarize_en(SHORT_EN_TEXT, max_sentences=3)
    print(f"–ö–æ—Ä–æ—Ç–∫–∏–π EN —Å–∞–º–º–∞—Ä–∏: {short_summary}")
    
    # –¢–µ–∫—Å—Ç —Å–æ —Å—Å—ã–ª–∫–∞–º–∏
    links_summary = summarize_en(TEXT_WITH_LINKS, max_sentences=3)
    print(f"–°–∞–º–º–∞—Ä–∏ —Å —Å—Å—ã–ª–∫–∞–º–∏: {links_summary}")
    assert 'https://' in links_summary, "–°—Å—ã–ª–∫–∏ –¥–æ–ª–∂–Ω—ã —Å–æ—Ö—Ä–∞–Ω—è—Ç—å—Å—è"
    
    print("‚úì –ê–Ω–≥–ª–∏–π—Å–∫–∏–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ\n")

async def test_full_summarizer():
    """–¢–µ—Å—Ç –ø–æ–ª–Ω–æ–≥–æ —Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä–∞ —Å —Ä–æ—É—Ç–∏–Ω–≥–æ–º"""
    print("=== –¢–µ—Å—Ç –ø–æ–ª–Ω–æ–≥–æ —Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä–∞ ===")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä (–±–µ–∑ Groq API –¥–ª—è —Ç–µ—Å—Ç–∞)
    summarizer = TextSummarizer(groq_api_key=None, use_local_fallback=False)
    
    # –†—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç (–¥–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å None –±–µ–∑ Groq API)
    ru_summary = await summarizer.summarize_text(RU_TEXT, target_ratio=0.3)
    print(f"RU —Å–∞–º–º–∞—Ä–∏: {ru_summary}")
    
    # –ê–Ω–≥–ª–∏–π—Å–∫–∏–π —Ç–µ–∫—Å—Ç (–¥–æ–ª–∂–µ–Ω –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç–∫—Å—Ç—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä)
    en_summary = await summarizer.summarize_text(EN_TEXT, target_ratio=0.3)
    print(f"EN —Å–∞–º–º–∞—Ä–∏ –¥–ª–∏–Ω–æ–π {len(en_summary) if en_summary else 0} —Å–∏–º–≤–æ–ª–æ–≤:")
    print(en_summary)
    
    if en_summary:
        assert len(en_summary) > 50, "EN —Å–∞–º–º–∞—Ä–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ"
        assert '‚Ä¢' in en_summary, "EN —Å–∞–º–º–∞—Ä–∏ –¥–æ–ª–∂–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –±—É–ª–ª–µ—Ç—ã"
    
    print("‚úì –ü–æ–ª–Ω—ã–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ\n")

def test_helper_functions():
    """–¢–µ—Å—Ç –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π"""
    print("=== –¢–µ—Å—Ç –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π ===")
    
    assert is_ru(RU_TEXT), "is_ru –¥–æ–ª–∂–Ω–∞ –≤–µ—Ä–Ω—É—Ç—å True –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"
    assert not is_ru(EN_TEXT), "is_ru –¥–æ–ª–∂–Ω–∞ –≤–µ—Ä–Ω—É—Ç—å False –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"
    
    assert is_en(EN_TEXT), "is_en –¥–æ–ª–∂–Ω–∞ –≤–µ—Ä–Ω—É—Ç—å True –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞" 
    assert not is_en(RU_TEXT), "is_en –¥–æ–ª–∂–Ω–∞ –≤–µ—Ä–Ω—É—Ç—å False –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"
    
    print("‚úì –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ\n")

async def main():
    """–ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤"""
    print("üß™ Smoke Test: –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ—Å—Ç–∏ (RU/EN)\n")
    
    try:
        await test_language_detection()
        test_english_summarizer()
        await test_full_summarizer()
        test_helper_functions()
        
        print("üéâ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ—à–ª–∏ —É—Å–ø–µ—à–Ω–æ!")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–∞—Ö: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())