#!/usr/bin/env python3
"""
–¢–µ—Å—Ç—ã –¥–ª—è –≤–µ–±-—ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
"""

import os
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
sys.path.insert(0, str(Path(__file__).parent.parent))

from content_extraction.web_extractor import _extract_content, _normalize_text, _extract_links_from_html


def load_fixture(filename: str) -> str:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç HTML —Ñ–∏–∫—Å—Ç—É—Ä—É –∏–∑ —Ñ–∞–π–ª–∞"""
    fixture_path = Path(__file__).parent / "fixtures" / filename
    with open(fixture_path, 'r', encoding='utf-8') as f:
        return f.read()


def test_news_extraction():
    """–¢–µ—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ –Ω–æ–≤–æ—Å—Ç–Ω–æ–π —Å—Ç–∞—Ç—å–∏"""
    html = load_fixture("news.html")
    
    page = _extract_content("https://example.com/news", "https://example.com/news", html)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∏–∑–≤–ª–µ—á–µ–Ω –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π –æ–±—ä–µ–º —Ç–µ–∫—Å—Ç–∞
    assert len(page.text) >= 800, f"–ò–∑–≤–ª–µ—á–µ–Ω–æ —Ç–æ–ª—å–∫–æ {len(page.text)} —Å–∏–º–≤–æ–ª–æ–≤, –æ–∂–∏–¥–∞–ª–æ—Å—å >= 800"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
    assert page.title is not None, "–ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –∏–∑–≤–ª–µ—á–µ–Ω"
    assert "—Ä–µ–≤–æ–ª—é—Ü–∏—è" in page.title.lower() or "–∏–∏" in page.title.lower(), f"–ù–µ–≤–µ—Ä–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫: {page.title}"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Å—ã–ª–∫–∏
    assert len(page.links) >= 3, f"–ù–∞–π–¥–µ–Ω–æ —Ç–æ–ª—å–∫–æ {len(page.links)} —Å—Å—ã–ª–æ–∫, –æ–∂–∏–¥–∞–ª–æ—Å—å >= 3"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Å—ã–ª–∫–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã
    for link in page.links:
        assert link['href'].startswith('https://'), f"–ù–µ–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Å—Å—ã–ª–∫–∞: {link['href']}"
        assert len(link['text']) > 3, f"–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç —Å—Å—ã–ª–∫–∏: {link['text']}"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    assert 'og:title' in page.meta or 'description' in page.meta, "–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –Ω–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã"
    
    print(f"‚úÖ News extraction: {len(page.text)} —Å–∏–º–≤–æ–ª–æ–≤, {len(page.links)} —Å—Å—ã–ª–æ–∫")


def test_blog_extraction():
    """–¢–µ—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ –±–ª–æ–≥–∞ (fallback –Ω–∞ readability)"""
    html = load_fixture("blog.html")
    
    page = _extract_content("https://example.com/blog", "https://example.com/blog", html)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    assert len(page.text) >= 500, f"–ò–∑–≤–ª–µ—á–µ–Ω–æ —Ç–æ–ª—å–∫–æ {len(page.text)} —Å–∏–º–≤–æ–ª–æ–≤, –æ–∂–∏–¥–∞–ª–æ—Å—å >= 500"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤ —Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    assert "python" in page.text.lower(), "–û—Å–Ω–æ–≤–Ω–∞—è —Ç–µ–º–∞ —Å—Ç–∞—Ç—å–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Ç–µ–∫—Å—Ç–µ"
    assert "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ" in page.text.lower() or "—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫" in page.text.lower(), "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
    assert page.title is not None, "–ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –∏–∑–≤–ª–µ—á–µ–Ω"
    
    print(f"‚úÖ Blog extraction: {len(page.text)} —Å–∏–º–≤–æ–ª–æ–≤")


def test_portal_with_noise():
    """–¢–µ—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –ø–æ—Ä—Ç–∞–ª–∞ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –º—É—Å–æ—Ä–∞"""
    html = load_fixture("portal.html")
    
    page = _extract_content("https://example.com/portal", "https://example.com/portal", html)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∏–∑–≤–ª–µ—á–µ–Ω –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
    assert len(page.text) >= 350, f"–ò–∑–≤–ª–µ—á–µ–Ω–æ —Ç–æ–ª—å–∫–æ {len(page.text)} —Å–∏–º–≤–æ–ª–æ–≤, –æ–∂–∏–¥–∞–ª–æ—Å—å >= 350"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º—É—Å–æ—Ä –Ω–µ –ø–æ–ø–∞–ª –≤ –∫–æ–Ω—Ç–µ–Ω—Ç
    noise_words = ["—Ä–µ–∫–ª–∞–º–∞", "–ø–æ–¥–ø–∏—Å–∞—Ç—å—Å—è", "–ø–æ–¥–µ–ª–∏—Ç—å—Å—è", "–Ω–∞–≤–∏–≥–∞—Ü–∏—è", "–ø–æ–≥–æ–¥–∞"]
    text_lower = page.text.lower()
    
    for noise_word in noise_words:
        assert noise_word not in text_lower or text_lower.count(noise_word) <= 1, \
            f"–ù–∞–π–¥–µ–Ω –º—É—Å–æ—Ä–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç: {noise_word}"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç
    assert "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏" in text_lower, "–û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω"
    assert "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç" in text_lower, "–ö–ª—é—á–µ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
    
    print(f"‚úÖ Portal extraction: {len(page.text)} —Å–∏–º–≤–æ–ª–æ–≤, –º—É—Å–æ—Ä –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω")


def test_short_content_error():
    """–¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ—Ä–æ—Ç–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    html = load_fixture("short.html")
    
    try:
        page = _extract_content("https://example.com/short", "https://example.com/short", html)
        # –ï—Å–ª–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–æ—à–ª–æ, –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∫–æ–Ω—Ç–µ–Ω—Ç –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∫–æ—Ä–æ—Ç–∫–∏–π
        assert len(page.text) < 350, "–ö–æ—Ä–æ—Ç–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –¥–æ–ª–∂–µ–Ω –ø—Ä–æ—Ö–æ–¥–∏—Ç—å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥"
    except Exception as e:
        # –û–∂–∏–¥–∞–µ–º –æ—à–∏–±–∫—É –æ –∫–æ—Ä–æ—Ç–∫–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–µ
        assert "–Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å" in str(e).lower(), f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}"
        print("‚úÖ Short content correctly rejected")
        return
    
    print("‚ö†Ô∏è Short content was extracted, but it's very short")


def test_text_normalization():
    """–¢–µ—Å—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞"""
    test_cases = [
        # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        ("–≠—Ç–æ   —Ç–µ–∫—Å—Ç  —Å    –±–æ–ª—å—à–∏–º–∏     –ø—Ä–æ–±–µ–ª–∞–º–∏", "–≠—Ç–æ —Ç–µ–∫—Å—Ç —Å –±–æ–ª—å—à–∏–º–∏ –ø—Ä–æ–±–µ–ª–∞–º–∏"),
        
        # –ù–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        ("–¢–µ–∫—Å—Ç\xa0—Å\xa0–Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–º–∏\xa0–ø—Ä–æ–±–µ–ª–∞–º–∏", "–¢–µ–∫—Å—Ç —Å –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–º–∏ –ø—Ä–æ–±–µ–ª–∞–º–∏"),
        
        # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–≤–æ–¥—ã —Å—Ç—Ä–æ–∫
        ("–ê–±–∑–∞—Ü 1\n\n\n\n–ê–±–∑–∞—Ü 2", "–ê–±–∑–∞—Ü 1\n\n–ê–±–∑–∞—Ü 2"),
        
        # –ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç
        ("", ""),
        
        # –¢–æ–ª—å–∫–æ –ø—Ä–æ–±–µ–ª—ã
        ("   \n\n   ", ""),
    ]
    
    for input_text, expected in test_cases:
        result = _normalize_text(input_text)
        assert result == expected, f"–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å. –í—Ö–æ–¥: {repr(input_text)}, –æ–∂–∏–¥–∞–ª–æ—Å—å: {repr(expected)}, –ø–æ–ª—É—á–µ–Ω–æ: {repr(result)}"
    
    print("‚úÖ Text normalization works correctly")


def test_link_normalization():
    """–¢–µ—Å—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Å—Å—ã–ª–æ–∫"""
    html = """
    <html>
    <body>
        <a href="https://example.com/full">–ü–æ–ª–Ω–∞—è —Å—Å—ã–ª–∫–∞</a>
        <a href="/relative">–û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è —Å—Å—ã–ª–∫–∞</a>
        <a href="#anchor">–Ø–∫–æ—Ä—å</a>
        <a href="mailto:test@example.com">Email</a>
        <a href="tel:+123456789">–¢–µ–ª–µ—Ñ–æ–Ω</a>
        <a href="javascript:void(0)">JavaScript</a>
        <a href="http://another.com/path">–î—Ä—É–≥–æ–π –¥–æ–º–µ–Ω</a>
        <a href="/short" title="–ö–æ—Ä–æ—Ç–∫–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫">OK</a>
        <a href="/empty"></a>
    </body>
    </html>
    """
    
    links = _extract_links_from_html(html, "https://example.com/page")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–ª–∏–¥–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
    assert len(links) >= 3, f"–ù–∞–π–¥–µ–Ω–æ —Ç–æ–ª—å–∫–æ {len(links)} –≤–∞–ª–∏–¥–Ω—ã—Ö —Å—Å—ã–ª–æ–∫"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é
    relative_link = next((link for link in links if "relative" in link['href']), None)
    assert relative_link is not None, "–û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è —Å—Å—ã–ª–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
    assert relative_link['href'] == "https://example.com/relative", f"–ù–µ–≤–µ—Ä–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: {relative_link['href']}"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é —è–∫–æ—Ä–µ–π, mailto, tel
    for link in links:
        assert not link['href'].startswith('#'), "–Ø–∫–æ—Ä—å –Ω–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω"
        assert not link['href'].startswith('mailto:'), "Email –Ω–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω"
        assert not link['href'].startswith('tel:'), "–¢–µ–ª–µ—Ñ–æ–Ω –Ω–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω"
        assert not link['href'].startswith('javascript:'), "JavaScript –Ω–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω"
        assert len(link['text']) > 3, f"–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç —Å—Å—ã–ª–∫–∏: {link['text']}"
    
    print(f"‚úÖ Link normalization: {len(links)} –≤–∞–ª–∏–¥–Ω—ã—Ö —Å—Å—ã–ª–æ–∫")


def run_all_tests():
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –≤—Å–µ —Ç–µ—Å—Ç—ã"""
    tests = [
        test_news_extraction,
        test_blog_extraction,
        test_portal_with_noise,
        test_short_content_error,
        test_text_normalization,
        test_link_normalization
    ]
    
    passed = 0
    failed = 0
    
    for test in tests:
        try:
            test()
            passed += 1
        except Exception as e:
            print(f"‚ùå {test.__name__}: {e}")
            failed += 1
    
    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–æ–≤: {passed} –ø—Ä–æ—à–ª–∏, {failed} –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å")
    return failed == 0


if __name__ == "__main__":
    import sys
    success = run_all_tests()
    sys.exit(0 if success else 1)