
Задача: добавить обработку аудиофайлов и голосовых сообщений с транскрибацией через мою существующую Whisper‑интеграцию и передачей результата в уже используемую Llama‑суммаризацию. Текущие функции суммаризации не менять.

Цель

При получении voice, audio (и опц. video_note) — скачать → при необходимости перекодировать → транскрибировать моим Whisper → передать в существующую текстовую суммаризацию (Llama) → вернуть краткую саммари + файл полной транскрипции.

Ограничение

Нельзя ломать текущую архитектуру и менять сигнатуры существующих функций суммаризации. Любая интеграция — через адаптеры и новые модули.

Новые файлы (минимально инвазивно)
	•	audio_pipeline/__init__.py
	•	audio_pipeline/downloader.py — скачивание файла из Telegram
	•	audio_pipeline/segmenter.py — VAD/чанкинг, если понадобится
	•	audio_pipeline/transcriber_adapter.py — адаптер к моему уже существующему Whisper-коду
	•	если у меня есть функция вида transcribe_audio(path, **kwargs) -> str|dict, оберни её тут, НЕ меняя оригинал
	•	если сигнатура другая — сделай тонкий слой, приводящий к единому интерфейсу
	•	audio_pipeline/handler.py — общий обработчик аудио/войсов
	•	utils/ffmpeg.py — безопасное перекодирование (OGG/OPUS/AMR/WEBM → WAV 16k mono)

Использование существующих модулей
	1.	Whisper:
	•	Найди существующий модуль/функцию Whisper (пробеги по проекту). Не трогай реализацию.
	•	В transcriber_adapter.py реализуй функцию:

def transcribe_with_whisper(src_wav_path: str, language_hint: str | None = None) -> dict:
    """
    Возвращает словарь:
    { "text": str, "language": str | None, "duration_sec": float | None, "segments": list|None, "avg_prob": float|None }
    """


	•	Если мой текущий Whisper не принимает WAV 16k mono — адаптируй вход (перекодируй заранее).

	2.	Llama (существующая суммаризация):
	•	Найди текущую функцию суммаризации текста и вызывай её как есть.
	•	Если имя/сигнатура нестандартные — создай адаптер summarization_adapter.py:

def run_summarization(text: str, language_hint: str | None = None, max_sentences: int | None = None) -> str:
    # внутри вызвать мой текущий суммаризатор (Llama)


	•	Не меняй мой текущий код суммаризации.

Поддерживаемые входы
	•	message.voice (OGG/OPUS)
	•	message.audio (MP3, M4A/AAC, WAV, FLAC)
	•	(опц.) message.video_note (извлечь аудиодорожку)
	•	Дополнительно через перекодирование: AMR, WEBM

Пайплайн
	1.	Получить файл → downloader.download_audio(file_id, out_dir) -> str (путь к исходнику).
	2.	utils/ffmpeg.to_wav_16k_mono(src, dst) — нормализовать для Whisper.
	3.	(Опц.) segmenter.segment_audio(wav_path, enable_vad, target_len_sec, overlap_sec) — если файл длинный.
	4.	transcriber_adapter.transcribe_with_whisper(...):
	•	Если сегменты — собрать общий full_text, язык, длительность, среднюю уверенность (если доступно).
	5.	summarization_adapter.run_summarization(full_text, language_hint=meta["language"]) — отправить в мою Llama‑суммаризацию.
	6.	Ответ пользователю: краткая саммари (3–8 пунктов) + документ с полной транскрипцией (.txt), + метаданные (язык, длительность).

Регистрация хендлеров (не менять существующие)

В основном модуле бота добавить (без правок текущих хендлеров текста/доков):

async def on_voice(update, context): return await audio_pipeline.handler.handle_voice(update, context)
async def on_audio(update, context): return await audio_pipeline.handler.handle_audio(update, context)
async def on_video_note(update, context): return await audio_pipeline.handler.handle_video_note(update, context)  # опционально

Конфигурация (новые env, с дефолтами)

AUDIO_SUMMARY_ENABLED=true
ASR_VAD_ENABLED=true
ASR_MAX_DURATION_MIN=90
FFMPEG_PATH=ffmpeg

# Если моя Llama вызывается по HTTP — НЕ внедрять заново, использовать уже существующие настройки проекта.
# Если нет — ничего не добавлять. В любом случае суммаризация идёт через текущую функцию.

UX
	•	При получении аудио/voice — typing/upload_audio action и сообщение-заглушка:
«Получил аудио, обрабатываю… (≈{duration} сек)»
	•	Финальный ответ:
«Саммари аудио (язык: {language or “—”}, длительность: {mm:ss})»
• 3–8 маркеров ключевых тезисов.
• Приложить .txt с полной транскрипцией.
• Если ограничение по длительности/размеру — понятная ошибка с лимитом.

Технические детали
	•	utils/ffmpeg.py:

def to_wav_16k_mono(src_path: str, dst_path: str, ffmpeg_path: str = "ffmpeg") -> None: ...
def extract_audio(src_path: str, dst_path: str, ffmpeg_path: str = "ffmpeg") -> None: ...  # для video_note


	•	segmenter.py: если ASR_VAD_ENABLED=true и длительность > N минут — разбить на чанки 30–60 сек, overlap 5–10%. Если в проекте уже есть VAD — используй его через адаптер.
	•	Все временные файлы — в tmp/ или /tmp, гарантированная очистка в finally.

Логи/диагностика
	•	Логировать: входной формат, длительность, язык, общее время пайплайна, число чанков.
	•	Не логировать сам текст речи.

Тесты (юнит + интеграция)
	•	utils/ffmpeg_test.py: перекодирование и корректная ошибка, если ffmpeg не найден.
	•	transcriber_adapter_test.py: мок Whisper‑функции, проверка нормализации результата.
	•	Интеграционный тест: фейковый voice.ogg и audio.mp3 → транскрипция → вызов существующей Llama‑суммаризации через адаптер → корректный ответ.

Критерии приёмки
	•	voice (ogg/opus) и audio (mp3/m4a/wav) до 20 минут — стабильно обрабатываются.
	•	Суммаризация вызывается моей существующей Llama‑реализацией (без изменений её кода).
	•	Никаких регрессий в текущем текстовом функционале.
	•	Временные файлы удаляются.
