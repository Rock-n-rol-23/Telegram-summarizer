PROMPT FOR REPLIT AGENT — Инкремент 2 (улучшенный парсинг веб-страниц)

Задача: Усилить извлечение контента с веб-сайтов для существующей команды суммаризации ссылок. Сделать многоступенчатый пайплайн: trafilatura → readability-lxml → bs4-эвристики, нормализовать текст/ссылки/метаданные, добавить кэширование и устойчивые сетевые настройки. Ничего не ломать в публичном API бота — только заменить внутренний экстрактор.

1) Зависимости (добавить/проверить requirements.txt)

trafilatura==1.6.3

readability-lxml==0.8.1

lxml[html_clean]==5.3.0

beautifulsoup4==4.12.3

html5lib==1.1

httpx==0.27.0

charset-normalizer==3.3.2

(опц.) selectolax==0.3.21 — для быстрого парсинга, если потребуется

Если в проекте уже есть http-клиент — используем его, иначе добавить httpx.

2) Новый модуль экстракции

Создать content_extraction/web_extractor.py и пакет content_extraction/__init__.py.

Структура и контракт:

# content_extraction/web_extractor.py
from dataclasses import dataclass
from typing import List, Optional, Dict

@dataclass
class ExtractedPage:
    url: str
    final_url: str
    title: Optional[str]
    byline: Optional[str]
    published_at: Optional[str]   # ISO 8601 если найдено
    lang: Optional[str]
    text: str                     # очищенный основной текст
    links: List[Dict[str, str]]   # [{"text": "...", "href": "https://...", "title": "..."}]
    meta: Dict[str, str]          # og:title, og:site_name, description и т.п.
    word_count: int
    char_count: int

async def extract_url(url: str, *, accept_lang: str = "ru,en-US;q=0.9,en;q=0.8", timeout_s: float = 12.0) -> ExtractedPage:
    """Главная функция: скачивает страницу, извлекает основной контент многоступенчато, нормализует и возвращает ExtractedPage.
    Бросает свои осмысленные ошибки на сетевые/контентные кейсы."""

3) Сетевой слой (внутри extract_url)

Использовать httpx.AsyncClient с:

follow_redirects=True, макс. редиректов 5

таймаут: connect=5s, read=7s, write=7s, pool=5s (или общий 12s)

заголовки:

User-Agent: современный десктоп UA (строку захардкодить)

Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8

Accept-Language: из параметра accept_lang

Cache-Control: no-cache

Перед GET можно сделать легкий HEAD (не обязательно). Если Content-Type не содержит text/html, вернуть осмысленную ошибку (“Похоже, это не статья: …”).

Корректно определить final_url после редиректов.

Правильно декодировать контент: использовать response.encoding (httpx + charset-normalizer).

4) Многоступенчатая экстракция

Шаг A: trafilatura

Использовать trafilatura.extract(raw_html, url=final_url, include_comments=False, include_tables=False, output_format="txt").

Метаданные через trafilatura.metadata.extract_metadata.

Если результат есть и длина >= 800 символов — принять как основной текст.

Шаг B: readability-lxml

Document(raw_html) → summary() → очистить через lxml.html.clean (убрать script/style/nav/header/footer/aside).

Из заголовков взять title, из meta — og:title, og:site_name, description, даты (article:published_time, date, DC.date).

Если длина >= 500 — принять.

Шаг C: bs4-эвристики (fallback)

Суп из html5lib или lxml парсера.

Удалить блочки по CSS-классам/ID: header, footer, nav, aside, .ad, .ads, .social, .breadcrumbs, .subscribe, .newsletter.

Найти “самый длинный” текстовый контейнер (div, article, main, section) по количеству видимых символов; объединить параграфы <p>, заголовки <h1..h3>.

Минимальная длина для принятия: >= 350.

Если все шаги < порогов — вернуть ошибку “Не удалось извлечь полезный текст”.

5) Нормализация текста и ссылок

Текст:

убрать множественные пробелы/переводы строк → привести к абзацам, не длиннее ~1000 символов.

заменить \xa0 на обычные пробелы, тримминг.

удалить “поделиться”, “читать также” и подобные хвосты по шаблонам.

Ссылки:

собрать все <a> внутри принятого контента: text, href, title.

нормализовать href через urllib.parse.urljoin(final_url, href).

отфильтровать якоря/почту/телефон; оставить только http(s).

дедуп по href (сохранять первое text).

Язык:

если в HTML есть <html lang=".."> — использовать;

иначе оставить None (в проекте уже есть авто-детект; он сработает позже).

6) Интеграция с текущим пайплайном

Найти текущий модуль/функцию, что обрабатывает ссылки (например, link_processor.py, web_processor.py или внутри summarizer).

Заменить точку входа извлечения контента на вызов await extract_url(url, accept_lang=resolve_from_user_settings()).

На выход передавать ExtractedPage.text в существующий summarizer без изменения его интерфейса.

В итоговый ответ добавить компактный блок “Ссылки из статьи” (первые 5–10 нормализованных ссылок) — если это уже реализовано, просто подменить источник данных.

7) Кэширование (SQLite)

Создать простой кэш web_cache (если ещё нет):

web_cache(
  url_hash TEXT PRIMARY KEY,
  final_url TEXT,
  fetched_at INTEGER,           -- unix time
  etag TEXT,
  last_modified TEXT,
  text_hash TEXT,
  meta_json TEXT,
  text BLOB
)


url_hash = sha256(canonical_url).

TTL: 72 часа. Перед сетевым запросом — проверять кэш. Если есть валидная запись и не истёк TTL — возвращать её.

После успешной экстракции — класть в кэш.

(Опционально) сохранять ETag/Last-Modified и пробовать условные запросы.

8) Обработка ошибок и сообщения пользователю

Сетевые: timeout/DNS/SSL → “Не удалось загрузить страницу. Проверь ссылку или попробуйте позже.”

Неподдерживаемый Content-Type → “Похоже, это не статья (тип: …). Пришлите обычную веб-страницу.”

Пустой/короткий контент → “Не удалось извлечь полезный текст со страницы. Возможно, это динамический/защищённый контент.”

Все исключения логировать c url, final_url, временем, размером ответа.

9) Конфиг/ENV

Добавить в общий конфиг (использовать текущий лоадер):

WEB_ACCEPT_LANGUAGE=ru,en-US;q=0.9,en;q=0.8
WEB_FETCH_TIMEOUT_S=12
WEB_CACHE_TTL_H=72

10) Тесты/проверки (минимум)

Добавить tests/test_web_extractor.py c фикстурами (локальные HTML в tests/fixtures/):

news.html (статья с датой/og) → извлекается ≥800 символов, есть title, найдены ≥3 ссылки.

blog.html (без og, но с читаемой структурой) → readability fallback.

portal.html (много мусора) → bs4-эвристики работают.

short.html (почти пусто) → корректная ошибка “короткий контент”.

Проверка нормализации ссылок urljoin.

11) Acceptance Criteria (DoD)

Ссылки теперь стабильно парсятся на большинстве новостных/блоговых страниц; “мусор” (меню/подвал/соцкнопки) в текст не попадает.

Для валидной статьи итоговый текст ≥ 500–800 символов и передаётся в существующий summarizer без изменения его интерфейса.

Блок “Ссылки из статьи” показывает нормализованные уникальные URL (до 10).

Включено кэширование (повторная суммаризация одной и той же ссылки не делает новый сетевой запрос в течение TTL).

Корректные и дружелюбные сообщения об ошибках для сетевых/контентных кейсов.

Никакие существующие команды и функционал не сломаны.

12) Подсказки по реализации

Для “коротко/мусор” можно считать длину после очистки без URL/скриптов.

Для дат: искать meta[property="article:published_time"], meta[name="date"], time[datetime]; приводить к ISO 8601 при наличии.

Для озаглавливания: og:title > <title> > h1.

Не трогать суммаризатор и лимиты (≤10 предложений) — это следующий слой, уже работает.

После внедрения запустить локально и проверить 3–5 реальных ссылок (новости, блог, документация). Если страница требует JS для отрисовки, экстрактор должен корректно сообщить об ограничении, без падения.