"""
–£–º–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å —ç–∫—Å—Ç—Ä–∞–∫—Ç–∏–≤–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ–º
"""

import re
import logging
from typing import Dict, List, Tuple, Set
from collections import defaultdict, Counter

logger = logging.getLogger(__name__)

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
try:
    import razdel
    from sumy.parsers.plaintext import PlaintextParser
    from sumy.nlp.tokenizers import Tokenizer
    from sumy.summarizers.text_rank import TextRankSummarizer
    from sumy.summarizers.lex_rank import LexRankSummarizer
    import rutermextract
    from natasha import (
        Segmenter, MorphVocab, NewsEmbedding, NewsMorphTagger,
        NewsNERTagger, NewsSyntaxParser, Doc, NamesExtractor,
        DatesExtractor, MoneyExtractor
    )
    SUMMARIZATION_AVAILABLE = True
except ImportError as e:
    SUMMARIZATION_AVAILABLE = False
    logger.warning(f"–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {e}")

# –ö–ª—é—á–µ–≤—ã–µ —Ç—Ä–∏–≥–≥–µ—Ä—ã –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏
AGREEMENT_TRIGGERS = [
    '–¥–æ–≥–æ–≤–æ—Ä', '–¥–æ–≥–æ–≤–æ—Ä–∏–ª–∏—Å—å', '—Å–æ–≥–ª–∞—Å–∏–ª–∏—Å—å', '—Ä–µ—à–∏–ª–∏', '–æ–±–µ—â–∞–ª–∏',
    '–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ–º', '–ø—Ä–∏–Ω—è—Ç–æ', '—É—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ', '–æ–¥–æ–±—Ä–µ–Ω–æ', '–∑–∞–ø–∏—Å–∞–ª–∏—Å—å'
]

DEADLINE_TRIGGERS = [
    '—Å—Ä–æ–∫', '–¥–µ–¥–ª–∞–π–Ω', '–∫', '–¥–æ', '–∑–∞–≤—Ç—Ä–∞', '—Å–µ–≥–æ–¥–Ω—è', '—á–µ—Ä–µ–∑',
    '–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫', '–≤—Ç–æ—Ä–Ω–∏–∫', '—Å—Ä–µ–¥–∞', '—á–µ—Ç–≤–µ—Ä–≥', '–ø—è—Ç–Ω–∏—Ü–∞', '—Å—É–±–±–æ—Ç–∞', '–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ',
    '–ø–Ω', '–≤—Ç', '—Å—Ä', '—á—Ç', '–ø—Ç', '—Å–±', '–≤—Å',
    '—É—Ç—Ä–æ–º', '–¥–Ω–µ–º', '–≤–µ—á–µ—Ä–æ–º', '–≤—Ä–µ–º–µ–Ω–∏'
]

ACTION_TRIGGERS = [
    '–Ω—É–∂–Ω–æ', '–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ', '—Å–ª–µ–¥—É–µ—Ç', '–¥–æ–ª–∂–Ω—ã', '–ø–ª–∞–Ω–∏—Ä—É–µ–º', '–±—É–¥–µ–º',
    '—Å–¥–µ–ª–∞—Ç—å', '–≤—ã–ø–æ–ª–Ω–∏—Ç—å', '–ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å', '–æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞—Ç—å', '—Å–≤—è–∑–∞—Ç—å—Å—è',
    '—É—Ç–æ—á–Ω–∏—Ç—å', '–ø—Ä–æ–≤–µ—Ä–∏—Ç—å', '–ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å'
]

CONDITION_TRIGGERS = [
    '–µ—Å–ª–∏', '–ø—Ä–∏ —É—Å–ª–æ–≤–∏–∏', '–≤ —Å–ª—É—á–∞–µ', '–≤–æ–∑–º–æ–∂–Ω–æ', '–º–æ–∂–µ—Ç –±—ã—Ç—å',
    '–ø–æ –∂–µ–ª–∞–Ω–∏—é', '–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ', '–Ω–∞ –≤—ã–±–æ—Ä', '–ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏'
]

SPECIAL_TERMS = [
    '–±–µ—Å–ø–ª–∞—Ç–Ω–æ', '–ø–ª–∞—Ç–Ω–æ', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Ü–µ–Ω–∞', '—Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ',
    '–≥—Ä–∞—Ñ–∏–∫', '–≤—Ä–µ–º—è', '–ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å', '–∑–∞–Ω—è—Ç–∏–µ', '—É—Ä–æ–∫',
    '–≤—Å—Ç—Ä–µ—á–∞', '—Å–æ–±—Ä–∞–Ω–∏–µ', '–∑–≤–æ–Ω–æ–∫', '—Å–æ–∑–≤–æ–Ω'
]

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã natasha –¥–ª—è –ª–µ–Ω–∏–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
_natasha_ready = False
_segmenter = None
_morph_vocab = None
_emb = None
_morph_tagger = None
_ner_tagger = None
_dates_extractor = None
_money_extractor = None


def init_natasha():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã natasha"""
    global _natasha_ready, _segmenter, _morph_vocab, _emb, _morph_tagger, _ner_tagger, _dates_extractor, _money_extractor
    
    if _natasha_ready or not SUMMARIZATION_AVAILABLE:
        return
    
    try:
        logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è natasha...")
        
        _segmenter = Segmenter()
        _morph_vocab = MorphVocab()
        _emb = NewsEmbedding()
        _morph_tagger = NewsMorphTagger(_emb)
        _ner_tagger = NewsNERTagger(_emb)
        _dates_extractor = DatesExtractor()
        _money_extractor = MoneyExtractor()
        
        _natasha_ready = True
        logger.info("Natasha –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ natasha: {e}")


def extract_sentences(text: str) -> List[str]:
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é razdel"""
    if not SUMMARIZATION_AVAILABLE:
        # Fallback —Ä–∞–∑–±–∏–µ–Ω–∏–µ
        sentences = re.split(r'[.!?]+', text)
        return [s.strip() for s in sentences if s.strip()]
    
    try:
        sentences = razdel.sentenize(text)
        return [sent.text.strip() for sent in sentences if sent.text.strip()]
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ razdel, –∏—Å–ø–æ–ª—å–∑—É—é fallback: {e}")
        sentences = re.split(r'[.!?]+', text)
        return [s.strip() for s in sentences if s.strip()]


def extract_key_terms(text: str, limit: int = 20) -> List[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã —Å –ø–æ–º–æ—â—å—é rutermextract"""
    if not SUMMARIZATION_AVAILABLE:
        return []
    
    try:
        terms = rutermextract.TermExtractor()
        extracted = terms(text)
        
        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ—Ä–º–∏–Ω—ã —Å –≤—ã—Å–æ–∫–æ–π —á–∞—Å—Ç–æ—Ç–æ–π
        key_terms = []
        for term in extracted[:limit]:
            if term.count >= 2 or len(term.normalized) > 3:
                key_terms.append(term.normalized)
        
        return key_terms
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ rutermextract: {e}")
        return []


def extract_entities_and_dates(text: str) -> Dict:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –∏ –¥–∞—Ç—ã —Å –ø–æ–º–æ—â—å—é natasha"""
    init_natasha()
    
    entities = {
        'names': [],
        'organizations': [],
        'locations': [],
        'dates': [],
        'money': []
    }
    
    if not _natasha_ready:
        return entities
    
    try:
        doc = Doc(text)
        doc.segment(_segmenter)
        doc.tag_morph(_morph_tagger)
        doc.tag_ner(_ner_tagger)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º NER
        for span in doc.spans:
            if span.type == 'PER':
                entities['names'].append(span.text)
            elif span.type == 'ORG':
                entities['organizations'].append(span.text)
            elif span.type == 'LOC':
                entities['locations'].append(span.text)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—ã
        for match in _dates_extractor(text):
            entities['dates'].append(match.fact.as_json)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–µ–Ω—å–≥–∏
        for match in _money_extractor(text):
            entities['money'].append(match.fact.as_json)
            
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ natasha extraction: {e}")
    
    return entities


def categorize_sentences(sentences: List[str]) -> Dict[str, List[Tuple[int, str]]]:
    """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ —Ç–∏–ø–∞–º"""
    categories = {
        'agreements': [],      # –î–æ–≥–æ–≤–æ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        'deadlines': [],       # –°—Ä–æ–∫–∏
        'actions': [],         # –î–µ–π—Å—Ç–≤–∏—è
        'conditions': [],      # –£—Å–ª–æ–≤–∏—è
        'special_terms': [],   # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        'other': []           # –û—Å—Ç–∞–ª—å–Ω–æ–µ
    }
    
    for i, sentence in enumerate(sentences):
        sentence_lower = sentence.lower()
        categorized = False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é
        for trigger in AGREEMENT_TRIGGERS:
            if trigger in sentence_lower:
                categories['agreements'].append((i, sentence))
                categorized = True
                break
        
        if not categorized:
            for trigger in DEADLINE_TRIGGERS:
                if trigger in sentence_lower:
                    categories['deadlines'].append((i, sentence))
                    categorized = True
                    break
        
        if not categorized:
            for trigger in ACTION_TRIGGERS:
                if trigger in sentence_lower:
                    categories['actions'].append((i, sentence))
                    categorized = True
                    break
        
        if not categorized:
            for trigger in CONDITION_TRIGGERS:
                if trigger in sentence_lower:
                    categories['conditions'].append((i, sentence))
                    categorized = True
                    break
        
        if not categorized:
            for trigger in SPECIAL_TERMS:
                if trigger in sentence_lower:
                    categories['special_terms'].append((i, sentence))
                    categorized = True
                    break
        
        if not categorized:
            categories['other'].append((i, sentence))
    
    return categories


def extract_important_sentences(text: str, verbosity: str = "normal") -> List[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤–∞–∂–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é TextRank"""
    if not SUMMARIZATION_AVAILABLE:
        sentences = extract_sentences(text)
        # –ü—Ä–æ—Å—Ç–æ–π fallback - –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        count = {"short": 3, "normal": 6, "detailed": 10}.get(verbosity, 6)
        return sentences[:count]
    
    try:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        sentence_count = {
            "short": 4,
            "normal": 8, 
            "detailed": 14
        }.get(verbosity, 8)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º TextRank
        parser = PlaintextParser.from_string(text, Tokenizer("russian"))
        summarizer = TextRankSummarizer()
        
        summary_sentences = summarizer(parser.document, sentence_count)
        return [str(sentence) for sentence in summary_sentences]
        
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ TextRank, –∏—Å–ø–æ–ª—å–∑—É—é fallback: {e}")
        sentences = extract_sentences(text)
        count = {"short": 3, "normal": 6, "detailed": 10}.get(verbosity, 6)
        return sentences[:count]


def format_bullets(categories: Dict[str, List[Tuple[int, str]]], entities: Dict) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å–∞–º–º–∞—Ä–∏ –≤ –≤–∏–¥–µ –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞"""
    result = []
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã
    if categories['agreements']:
        result.append("ü§ù **–î–æ–≥–æ–≤–æ—Ä–µ–Ω–Ω–æ—Å—Ç–∏:**")
        for _, sentence in categories['agreements'][:3]:
            result.append(f"‚Ä¢ {sentence}")
        result.append("")
    
    # –°—Ä–æ–∫–∏
    if categories['deadlines']:
        result.append("‚è∞ **–°—Ä–æ–∫–∏ –∏ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ:**")
        for _, sentence in categories['deadlines'][:3]:
            result.append(f"‚Ä¢ {sentence}")
        result.append("")
    
    # –î–µ–π—Å—Ç–≤–∏—è
    if categories['actions']:
        result.append("‚úÖ **–°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:**")
        for _, sentence in categories['actions'][:3]:
            result.append(f"‚Ä¢ {sentence}")
        result.append("")
    
    # –£—Å–ª–æ–≤–∏—è
    if categories['conditions']:
        result.append("‚ùì **–£—Å–ª–æ–≤–∏—è –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:**")
        for _, sentence in categories['conditions'][:2]:
            result.append(f"‚Ä¢ {sentence}")
        result.append("")
    
    # –í–∞–∂–Ω—ã–µ —Ñ–∞–∫—Ç—ã
    fact_check = []
    if entities['dates']:
        fact_check.append(f"–î–∞—Ç—ã: {', '.join([str(d) for d in entities['dates'][:3]])}")
    if entities['money']:
        fact_check.append(f"–°—É–º–º—ã: {', '.join([str(m) for m in entities['money'][:2]])}")
    
    if fact_check:
        result.append("üîç **–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–∂–Ω–æ–≥–æ:**")
        result.append(f"‚Ä¢ {'; '.join(fact_check)}")
    
    return "\n".join(result)


def format_paragraph(important_sentences: List[str], entities: Dict) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å–∞–º–º–∞—Ä–∏ –≤ –≤–∏–¥–µ –∞–±–∑–∞—Ü–µ–≤"""
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ 2-3 –∞–±–∑–∞—Ü–∞
    paragraphs = []
    
    # –ü–µ—Ä–≤—ã–π –∞–±–∑–∞—Ü - –æ—Å–Ω–æ–≤–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
    if len(important_sentences) >= 3:
        paragraphs.append(" ".join(important_sentences[:3]))
    
    # –í—Ç–æ—Ä–æ–π –∞–±–∑–∞—Ü - –¥–µ—Ç–∞–ª–∏ –∏ —Å–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏
    if len(important_sentences) >= 6:
        paragraphs.append(" ".join(important_sentences[3:6]))
    
    # –¢—Ä–µ—Ç–∏–π –∞–±–∑–∞—Ü - –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    if len(important_sentences) > 6:
        paragraphs.append(" ".join(important_sentences[6:]))
    
    result = "\n\n".join(paragraphs)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –≤–∞–∂–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤
    fact_check = []
    if entities['dates']:
        fact_check.append(f"–¥–∞—Ç—ã: {', '.join([str(d) for d in entities['dates'][:3]])}")
    if entities['money']:
        fact_check.append(f"—Å—É–º–º—ã: {', '.join([str(m) for m in entities['money'][:2]])}")
    
    if fact_check:
        result += f"\n\n**–í–∞–∂–Ω—ã–µ —Ñ–∞–∫—Ç—ã:** {'; '.join(fact_check)}."
    
    return result


def format_structured(categories: Dict[str, List[Tuple[int, str]]], important_sentences: List[str], entities: Dict, verbosity: str) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å–∞–º–º–∞—Ä–∏ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –≤–∏–¥–µ"""
    result = []
    
    # –ö—Ä–∞—Ç–∫–∏–π –∏—Ç–æ–≥ –≤ 1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    if important_sentences:
        summary = " ".join(important_sentences[:2])
        result.append(f"üìã **–ö—Ä–∞—Ç–∫–∏–π –∏—Ç–æ–≥:** {summary}")
        result.append("")
    
    # –î–æ–≥–æ–≤–æ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
    if categories['agreements']:
        result.append("ü§ù **–ö–ª—é—á–µ–≤—ã–µ –¥–æ–≥–æ–≤–æ—Ä–µ–Ω–Ω–æ—Å—Ç–∏:**")
        count = 2 if verbosity == "short" else 3 if verbosity == "normal" else 4
        for _, sentence in categories['agreements'][:count]:
            result.append(f"‚Ä¢ {sentence}")
        result.append("")
    
    # –°—Ä–æ–∫–∏ –∏ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ
    deadline_items = categories['deadlines'] + categories['special_terms']
    if deadline_items:
        result.append("‚è∞ **–°—Ä–æ–∫–∏ –∏ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ:**")
        count = 2 if verbosity == "short" else 3 if verbosity == "normal" else 4
        for _, sentence in deadline_items[:count]:
            result.append(f"‚Ä¢ {sentence}")
        result.append("")
    
    # –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏
    if categories['actions']:
        result.append("‚úÖ **–°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:**")
        count = 2 if verbosity == "short" else 3 if verbosity == "normal" else 5
        for _, sentence in categories['actions'][:count]:
            result.append(f"‚Ä¢ {sentence}")
        result.append("")
    
    # –û—Ç–∫—Ä—ã—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã
    if categories['conditions'] and verbosity != "short":
        result.append("‚ùì **–û—Ç–∫—Ä—ã—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã:**")
        for _, sentence in categories['conditions'][:2]:
            result.append(f"‚Ä¢ {sentence}")
        result.append("")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–∂–Ω–æ–≥–æ
    fact_check = []
    if entities.get('dates'):
        dates_str = ', '.join([str(d) for d in entities['dates'][:3]])
        fact_check.append(f"–î–∞—Ç—ã/–≤—Ä–µ–º—è: {dates_str}")
    
    if entities.get('money'):
        money_str = ', '.join([str(m) for m in entities['money'][:2]])
        fact_check.append(f"–°—É–º–º—ã: {money_str}")
    
    # –ò—â–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–∞–∂–Ω—ã–µ —Ñ–∞–∫—Ç—ã –≤ —Ç–µ–∫—Å—Ç–µ
    important_facts = []
    all_text = " ".join([sent for _, sent in sum(categories.values(), [])])
    
    if re.search(r'\b–±–µ—Å–ø–ª–∞—Ç–Ω\w*', all_text, re.IGNORECASE):
        important_facts.append("–±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ —É—Å–ª–æ–≤–∏—è")
    if re.search(r'\b—Ä–∞—Å–ø–∏—Å–∞–Ω–∏\w*', all_text, re.IGNORECASE):
        important_facts.append("–≤–æ–ø—Ä–æ—Å—ã —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è")
    if re.search(r'\b–ø–µ—Ä–≤–æ–µ –∑–∞–Ω—è—Ç–∏–µ\b', all_text, re.IGNORECASE):
        important_facts.append("–ø—Ä–æ–±–Ω–æ–µ –∑–∞–Ω—è—Ç–∏–µ")
    
    if fact_check or important_facts:
        result.append("üîç **–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–∂–Ω–æ–≥–æ:**")
        if fact_check:
            result.append(f"‚Ä¢ {'; '.join(fact_check)}")
        if important_facts:
            result.append(f"‚Ä¢ –û—Ç–º–µ—á–µ–Ω–æ: {', '.join(important_facts)}")
    
    return "\n".join(result)


def smart_summarize(transcript: Dict, format: str = "structured", verbosity: str = "normal") -> str:
    """
    –£–º–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞
    
    Args:
        transcript: —Ä–µ–∑—É–ª—å—Ç–∞—Ç transcribe_audio
        format: "structured" | "bullets" | "paragraph" 
        verbosity: "short" | "normal" | "detailed"
    
    Returns:
        –ì–æ—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç —Å–∞–º–º–∞—Ä–∏
    """
    
    if not SUMMARIZATION_AVAILABLE:
        # –ü—Ä–æ—Å—Ç–æ–π fallback
        text = transcript.get("text", "")
        sentences = extract_sentences(text)
        count = {"short": 3, "normal": 5, "detailed": 8}.get(verbosity, 5)
        summary_sentences = sentences[:count]
        return "\n‚Ä¢ ".join(summary_sentences)
    
    text = transcript.get("text", "")
    if not text or len(text) < 50:
        return "–°–ª–∏—à–∫–æ–º –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"
    
    try:
        logger.info(f"–ù–∞—á–∏–Ω–∞—é —É–º–Ω—É—é —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤, —Ñ–æ—Ä–º–∞—Ç={format}, –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç—å={verbosity}")
        
        # 1. –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = extract_sentences(text)
        if len(sentences) < 2:
            return text
        
        # 2. –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        categories = categorize_sentences(sentences)
        
        # 3. –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–∞–∂–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ TextRank
        important_sentences = extract_important_sentences(text, verbosity)
        
        # 4. –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ –∏ –¥–∞—Ç—ã
        entities = extract_entities_and_dates(text)
        
        # 5. –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if format == "bullets":
            result = format_bullets(categories, entities)
        elif format == "paragraph":
            result = format_paragraph(important_sentences, entities)
        else:  # structured
            result = format_structured(categories, important_sentences, entities, verbosity)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É
        if len(result) < 100:
            # –î–æ–±–∞–≤–ª—è–µ–º –µ—â–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            additional = important_sentences[len(important_sentences)//2:]
            if format == "paragraph":
                result += "\n\n" + " ".join(additional[:3])
            else:
                result += "\n\nüìÑ **–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ:**\n‚Ä¢ " + "\n‚Ä¢ ".join(additional[:3])
        
        logger.info(f"–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(result)} —Å–∏–º–≤–æ–ª–æ–≤")
        return result
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —É–º–Ω–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏: {e}")
        
        # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
        sentences = extract_sentences(text)
        count = {"short": 4, "normal": 8, "detailed": 12}.get(verbosity, 8)
        summary_sentences = sentences[:count]
        
        if format == "bullets":
            return "‚Ä¢ " + "\n‚Ä¢ ".join(summary_sentences)
        else:
            return "\n\n".join([" ".join(summary_sentences[i:i+3]) for i in range(0, len(summary_sentences), 3)])


def check_summarization_availability() -> Dict[str, bool]:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"""
    availability = {
        "razdel": False,
        "sumy": False,
        "rutermextract": False,
        "natasha": False
    }
    
    try:
        import razdel
        availability["razdel"] = True
    except ImportError:
        pass
    
    try:
        from sumy.summarizers.text_rank import TextRankSummarizer
        availability["sumy"] = True
    except ImportError:
        pass
    
    try:
        import rutermextract
        availability["rutermextract"] = True
    except ImportError:
        pass
    
    try:
        from natasha import Segmenter
        availability["natasha"] = True
    except ImportError:
        pass
    
    return availability


def get_summarization_info() -> str:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"""
    availability = check_summarization_availability()
    
    available_count = sum(availability.values())
    total_count = len(availability)
    
    if available_count == total_count:
        return "–£–º–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è"
    elif available_count >= 2:
        return f"–£–º–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —á–∞—Å—Ç–∏—á–Ω–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è ({available_count}/{total_count} –º–æ–¥—É–ª–µ–π)"
    else:
        return "–£–º–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ - —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ razdel, sumy, rutermextract, natasha"